{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Projeto LLM - Modelo Gemma**"
      ],
      "metadata": {
        "id": "3yWJCEWeor0j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbDneersoj5m"
      },
      "outputs": [],
      "source": [
        "# Instalando pacotes\n",
        "#!pip install -U keras-nlp\n",
        "#!pip install -U keras\n",
        "#!pip install gemma"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalando pacote\n",
        "#!pip install huggingface-hub"
      ],
      "metadata": {
        "id": "7Y6l1fDbUeVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importando bibliotecas\n",
        "\n",
        "# bibliotecas sistema\n",
        "import re\n",
        "import string\n",
        "import collections\n",
        "from collections import Counter\n",
        "\n",
        "# Biblioteca para manipulação dados\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Biblioteca visualização de dados\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Biblioteca NLP\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Biblioteca mensagens de alertas\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "mgCtYheGopm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Versão keras tensorflow\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "print(\"Versão tensorflow:\", tf.__version__)\n",
        "print(\"Versão keras:\", keras.__version__)"
      ],
      "metadata": {
        "id": "9wRkYF2n2s7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Versão GPU\n",
        "!nvcc --version"
      ],
      "metadata": {
        "id": "movS9w1gqMiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tipo GPU\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "Awuh42KfqM1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifique se a GPU está disponível\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Dispositivo disponível:\", device)"
      ],
      "metadata": {
        "id": "6bGTZENO2q6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Base dados\n",
        "df = pd.read_csv('/content/Tweets.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "vrCRp6Ndoppl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizando 5 primeiros dados\n",
        "df.head()"
      ],
      "metadata": {
        "id": "bed32O08opsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizando 5 últimos dados\n",
        "df.tail()"
      ],
      "metadata": {
        "id": "xHD4w0DhqamR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualziando linhas e colunas\n",
        "df.shape"
      ],
      "metadata": {
        "id": "8_Z8wi_JqbcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Pré-processamento**"
      ],
      "metadata": {
        "id": "gstzHFlM0Vma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Info dados\n",
        "df.info()"
      ],
      "metadata": {
        "id": "Nrn0bycrqcxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tipo de dados\n",
        "df.dtypes"
      ],
      "metadata": {
        "id": "BOgXO8eRqdi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converter todos os valores da coluna \"clean_text\" para strings\n",
        "df['text'] = df['text'].astype(str)"
      ],
      "metadata": {
        "id": "F303ZIO81dkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converter todos os valores da coluna \"clean_text\" para strings\n",
        "df['selected_text'] = df['selected_text'].astype(str)"
      ],
      "metadata": {
        "id": "RSQBxxhb1dpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converter todos os valores da coluna \"clean_text\" para strings\n",
        "df['sentiment'] = df['sentiment'].astype(str)"
      ],
      "metadata": {
        "id": "CrJ4ICOu1dso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Excluir coluna\n",
        "df = df.drop(['textID'], axis=1)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "pqXjglBe4urw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para pré-processamento do texto\n",
        "def preprocess_text(text):\n",
        "    if isinstance(text, str):\n",
        "\n",
        "        # Verificar se o texto é uma string\n",
        "        # Converter texto para minúsculas\n",
        "        text = text.lower()\n",
        "\n",
        "        # Remover emojis\n",
        "        emoji_pattern = re.compile(\"[\"\n",
        "                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                                   u\"\\U0001F300-\\U0001F5FF\"  # símbolos & pictogramas\n",
        "                                   u\"\\U0001F680-\\U0001F6FF\"  # transporte & símbolos mapas\n",
        "                                   u\"\\U0001F1E0-\\U0001F1FF\"  # bandeiras (iOS)\n",
        "                                   u\"\\U00002500-\\U00002BEF\"  # caracteres chineses/japoneses coreanos unificados\n",
        "                                   u\"\\U00002702-\\U000027B0\"\n",
        "                                   u\"\\U00002702-\\U000027B0\"\n",
        "                                   u\"\\U000024C2-\\U0001F251\"\n",
        "                                   u\"\\U0001f926-\\U0001f937\"\n",
        "                                   u\"\\U00010000-\\U0010ffff\"\n",
        "                                   u\"\\u2640-\\u2642\"\n",
        "                                   u\"\\u2600-\\u2B55\"\n",
        "                                   u\"\\u200d\"\n",
        "                                   u\"\\u23cf\"\n",
        "                                   u\"\\u23e9\"\n",
        "                                   u\"\\u231a\"\n",
        "                                   u\"\\ufe0f\"  # dingbats\n",
        "                                   u\"\\u3030\"\n",
        "                                   \"]+\", flags=re.UNICODE)\n",
        "        text = emoji_pattern.sub(r'', text)\n",
        "\n",
        "        # Remover caracteres especiais\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "        # Tokenizar o texto em palavras\n",
        "        tokens = word_tokenize(text)\n",
        "\n",
        "        # Remover stopwords\n",
        "        stop_words = set(stopwords.words('english')) # Defina o idioma apropriado\n",
        "        tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "        # Lematização (redução das palavras às suas formas base)\n",
        "        lemmatizer = WordNetLemmatizer() # Utilize o lematizador apropriado para o idioma\n",
        "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "        # Juntar as palavras de volta em texto\n",
        "        preprocessed_text = ' '.join(tokens)\n",
        "    else:\n",
        "        preprocessed_text = np.nan  # Caso não seja uma string, mantenha o valor como NaN\n",
        "\n",
        "    return preprocessed_text\n",
        "\n",
        "# Aplicar a função de pré-processamento à coluna \"clean_text\"\n",
        "df['clean_text_2'] = df['text'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "3dIet6i9q2F3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remover linhas com valores nulos na coluna 'clean_text_2'\n",
        "df = df.dropna(subset=['clean_text_2'])"
      ],
      "metadata": {
        "id": "khPiq_rU0b0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converter todos os valores da coluna \"clean_text\" para strings\n",
        "df['clean_text_2'] = df['clean_text_2'].astype(str)"
      ],
      "metadata": {
        "id": "jfbLKgECscMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenar todos os textos em uma única string\n",
        "all_text = ' '.join(df['clean_text_2'])"
      ],
      "metadata": {
        "id": "QPny8a1a1bhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exibir o DataFrame resultante\n",
        "df.head()"
      ],
      "metadata": {
        "id": "NM4NP8W-0cVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Modelo LLM - Gemma**"
      ],
      "metadata": {
        "id": "c7DI95V-zxYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GemmaModel, GemmaTokenizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans"
      ],
      "metadata": {
        "id": "QPKVNuu0Rt8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifique se o GPU está disponível\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "id": "a1RTKfzdRxn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# login huggingface\n",
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "QGAT0BuOLoFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Base dados\n",
        "df"
      ],
      "metadata": {
        "id": "hYlbD0AmCIE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importando biblioteca\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Codificação dos sentimentos em valores numéricos\n",
        "encoder = LabelEncoder()\n",
        "df['sentiment_encoded'] = encoder.fit_transform(df['sentiment'])\n",
        "\n",
        "# Visualziando dataset\n",
        "df"
      ],
      "metadata": {
        "id": "XjYW8PYODjYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Vetorizar os textos\n",
        "\n",
        "# Importando bilioteca\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Vetorizando\n",
        "vetorizador = TfidfVectorizer()\n",
        "\n",
        "# Treinamento\n",
        "X = vetorizador.fit_transform(df.clean_text_2)\n",
        "\n",
        "# Visualizando\n",
        "vetorizador"
      ],
      "metadata": {
        "id": "gqHaq5-WJ_mM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2Model, GPT2Tokenizer\n",
        "from transformers import GemmaModel, GemmaConfig, GemmaTokenizer"
      ],
      "metadata": {
        "id": "MQ-i08nPLAGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifique se a GPU está disponível\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "id": "WW-BloBGc8B-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Supondo que 'dataset' é o seu conjunto de dados\n",
        "texts = df['clean_text_2'].tolist()"
      ],
      "metadata": {
        "id": "k1vguvXrRaN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modelo LLM com GPU\n",
        "model = GPT2Model.from_pretrained(\"google/gemma-2b\").to(device)"
      ],
      "metadata": {
        "id": "PuJZk6acLAJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar o tokenizador e o modelo Gemma\n",
        "tokenizer = GemmaTokenizer.from_pretrained(\"google/gemma-2b\")"
      ],
      "metadata": {
        "id": "GEtx8XY8d_mA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizar e codificar os textos\n",
        "inputs = tokenizer(texts,\n",
        "                   return_tensors=\"pt\",\n",
        "                   padding=True,\n",
        "                   truncation=True,\n",
        "                   max_length=512)"
      ],
      "metadata": {
        "id": "WP8offaEPstN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# Configurações de batch e truncamento\n",
        "max_length = 128  # Define o tamanho máximo dos textos\n",
        "batch_size = 8    # Define o tamanho do batch\n",
        "\n",
        "# Calcula o número total de épocas\n",
        "total_epochs = math.ceil(len(texts) / batch_size)\n",
        "\n",
        "# Lista para armazenar as representações vetoriais\n",
        "text_embeddings = []\n",
        "\n",
        "# Processamento em lotes para reduzir a memória\n",
        "for epoch in range(total_epochs):\n",
        "    # Calcula o índice inicial e final do lote atual\n",
        "    start_index = epoch * batch_size\n",
        "    end_index = min((epoch + 1) * batch_size, len(texts))\n",
        "\n",
        "    # Extrai o lote atual de textos\n",
        "    batch_texts = texts[start_index:end_index]\n",
        "\n",
        "    # Tokeniza os textos\n",
        "    inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length).to(device)\n",
        "\n",
        "    # Obtém a representação vetorial dos textos\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Usa a última camada oculta como representação vetorial do texto\n",
        "    batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()  # Movendo para CPU para economizar memória\n",
        "\n",
        "    # Adiciona as representações vetoriais do lote à lista\n",
        "    text_embeddings.extend(batch_embeddings)\n",
        "\n",
        "    # Imprime treinamento\n",
        "    print(f\"Treinamento Modelo {epoch + 1}/{total_epochs} LLM\")\n",
        "\n",
        "# Convertendo para numpy array\n",
        "text_embeddings = np.array(text_embeddings)"
      ],
      "metadata": {
        "id": "vngUhKsEQJgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Encontrar o valor cluster\n",
        "\n",
        "# Importando biblioteca\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Calcula a inércia para diferentes números de clusters\n",
        "inertias = []\n",
        "max_clusters = 10\n",
        "for k in range(2, max_clusters + 1):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(text_embeddings)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "\n",
        "# Encontra o ponto de cotovelo\n",
        "diff = np.diff(inertias)\n",
        "diff_r = diff[1:] / diff[:-1]\n",
        "\n",
        "# Plotando o gráfico da inércia em função do número de clusters\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(2, max_clusters + 1), inertias, marker='o')\n",
        "plt.xlabel('Número de Clusters')\n",
        "plt.ylabel('Inércia')\n",
        "plt.title('Método do Cotovelo')\n",
        "plt.xticks(np.arange(2, max_clusters + 1, step=1))\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "\n",
        "# Encontrando o número ideal de clusters usando a regra do cotovelo\n",
        "optimal_k = np.argmax(diff_r) + 2  # Adiciona 2 porque começamos em k=2\n",
        "print(\"Número ideal de clusters (regra do cotovelo):\", optimal_k)"
      ],
      "metadata": {
        "id": "i_n0t53LMgk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Modelo kmeans\n",
        "\n",
        "# Clusterização com K-means usando o número ideal de clusters\n",
        "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
        "\n",
        "# Treinamento modelo\n",
        "kmeans.fit(text_embeddings)"
      ],
      "metadata": {
        "id": "5vqcLTxoMgsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Numero cluster\n",
        "clusters = kmeans.labels_"
      ],
      "metadata": {
        "id": "FBal28ZtMgvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adicione os clusters ao conjunto de dados\n",
        "df['cluster'] = clusters\n",
        "\n",
        "# Visualiznado\n",
        "df.head()"
      ],
      "metadata": {
        "id": "M-XnylaMMgx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize os resultados\n",
        "for cluster_num in range(optimal_k):\n",
        "    print(f\"Cluster {cluster_num}:\")\n",
        "    cluster_indices = df[df['cluster'] == cluster_num].index.tolist()\n",
        "    for idx in cluster_indices:\n",
        "        print(f\"Text: {texts[idx]}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "TmpaXBGKLTOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprimir os rótulos dos clusters e o número de clusters únicos\n",
        "print(\"Rótulos dos Clusters:\", kmeans.labels_)\n",
        "print(\"Número de Clusters Únicos:\", optimal_k)"
      ],
      "metadata": {
        "id": "_B4Hnuk1LTR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Avaliação da qualidade da clusterização\n",
        "num_clusters = len(np.unique(kmeans.labels_))\n",
        "if num_clusters > 1:\n",
        "    silhouette_avg = silhouette_score(X, kmeans.labels_)\n",
        "    print(\"Silhouette Score:\", silhouette_avg)\n",
        "else:\n",
        "    print(\"Não é possível calcular o escore de silhueta com apenas um cluster.\")"
      ],
      "metadata": {
        "id": "mWcSHmG2LTVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Importando biblioteca\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# Reduzir a dimensionalidade dos vetores de características usando o TruncatedSVD\n",
        "tsne = TruncatedSVD(n_components=2)\n",
        "X_tsne_Truncated_SVD = tsne.fit_transform(text_embeddings)\n",
        "\n",
        "# Reduzir a dimensionalidade dos vetores de características usando o PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(text_embeddings)\n",
        "\n",
        "# Reduzir a dimensionalidade dos vetores de características usando o t-SNE\n",
        "tsne = TSNE(n_components=2, perplexity=200)\n",
        "X_tsne = tsne.fit_transform(text_embeddings)"
      ],
      "metadata": {
        "id": "0sEQwATh2P_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.lines import Line2D\n",
        "\n",
        "# Função para criar legendas personalizadas\n",
        "def create_custom_legend(labels, colors, markers, title):\n",
        "    custom_legend = []\n",
        "    for label, color, marker in zip(labels, colors, markers):\n",
        "        custom_legend.append(Line2D([0], [0], marker=marker, color='w', label=label, markerfacecolor=color, markersize=10))\n",
        "    return custom_legend\n",
        "\n",
        "# Lista de cores e marcadores para cada sentimento\n",
        "colors_sentimentos = {'positivo': 'blue', 'negativo': 'red', 'neutro': 'green'}\n",
        "markers_sentimentos = {'positivo': 'o', 'negativo': 's', 'neutro': 'D'}\n",
        "\n",
        "# Plotar os clusters com cores diferentes para cada sentimento usando PCA, t-SNE e TruncatedSVD\n",
        "plt.figure(figsize=(18, 6))\n",
        "\n",
        "# Plotar os clusters usando PCA\n",
        "plt.subplot(1, 3, 1)\n",
        "sns.scatterplot(x=X_pca[:, 0],\n",
        "                y=X_pca[:, 1],\n",
        "                hue=df['cluster'],\n",
        "                style=df['sentiment'],\n",
        "                palette='tab10',  # Usando uma paleta padrão com 10 cores diferentes\n",
        "                markers=['o', 's', 'D'],  # Definindo marcadores padrão\n",
        "                legend='full')\n",
        "plt.title('Clusters de Textos com Sentimentos (PCA) - LLM Gemma')\n",
        "plt.xlabel('Componente 1 (PCA)')\n",
        "plt.ylabel('Componente 2 (PCA)')\n",
        "plt.legend(handles=create_custom_legend(legendas_sentimentos.values(), colors_sentimentos.values(), markers_sentimentos.values(), 'Sentimento'))\n",
        "\n",
        "# Plotar os clusters usando t-SNE\n",
        "plt.subplot(1, 3, 2)\n",
        "sns.scatterplot(x=X_tsne[:, 0],\n",
        "                y=X_tsne[:, 1],\n",
        "                hue=df['cluster'],\n",
        "                style=df['sentiment'],\n",
        "                palette='tab10',\n",
        "                markers=['o', 's', 'D'],\n",
        "                legend='full')\n",
        "plt.title('Clusters de Textos com Sentimentos (t-SNE) - LLM Gemma')\n",
        "plt.xlabel('Componente 1 (t-SNE)')\n",
        "plt.ylabel('Componente 2 (t-SNE)')\n",
        "plt.legend(handles=create_custom_legend(legendas_sentimentos.values(), colors_sentimentos.values(), markers_sentimentos.values(), 'Sentimento'))\n",
        "\n",
        "# Plotar os clusters usando TruncatedSVD\n",
        "plt.subplot(1, 3, 3)\n",
        "sns.scatterplot(x=X_tsne_Truncated_SVD[:, 0],\n",
        "                y=X_tsne_Truncated_SVD[:, 1],\n",
        "                hue=df['cluster'],\n",
        "                style=df['sentiment'],\n",
        "                palette='tab10',\n",
        "                markers=['o', 's', 'D'],\n",
        "                legend='full')\n",
        "plt.title('Clusters de Textos com Sentimentos (TruncatedSVD) - LLM Gemma')\n",
        "plt.xlabel('Componente 1 (TruncatedSVD)')\n",
        "plt.ylabel('Componente 2 (TruncatedSVD)')\n",
        "plt.legend(handles=create_custom_legend(legendas_sentimentos.values(), colors_sentimentos.values(), markers_sentimentos.values(), 'Sentimento'))\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# Salvar as imagens\n",
        "plt.savefig('clusters_pca_tsne_truncatedsvd.png')  # Nome do arquivo pode ser alterado conforme necessário\n",
        "\n",
        "# Visualizar os gráficos\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "amZAxIL1gb_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Salvando dataset\n",
        "df.to_csv('dataset_clusterizado_LLM.csv', index=False)"
      ],
      "metadata": {
        "id": "sWSrUFVpgdVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Salvando modelo\n",
        "\n",
        "# Importando biblioteca\n",
        "import joblib\n",
        "\n",
        "# Salvar o modelo KMeans em um arquivo\n",
        "joblib.dump(kmeans, 'modelo_kmeans_LLM.pkl')"
      ],
      "metadata": {
        "id": "cfmCrueYgd8p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}