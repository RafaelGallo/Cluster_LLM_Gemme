


# Instalando pacotes
!pip install -U keras-nlp
!pip install -U keras
!pip install gemma


# Instalando pacote
!pip install huggingface-hub


# Importando bibliotecas

# bibliotecas sistema
import re
import string
import collections
from collections import Counter

# Biblioteca para manipulação dados
import pandas as pd
import numpy as np

# Biblioteca visualização de dados
import matplotlib.pyplot as plt
import seaborn as sns

# Biblioteca NLP
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Biblioteca mensagens de alertas
import warnings
warnings.filterwarnings('ignore')

# Biblioteca para modelos LLM
import torch
import gemma
import keras_nlp
import tensorflow as tf
from tensorflow.keras.layers import Input, LSTM, Dense
from tensorflow.keras.models import Model


# Versão GPU
!nvcc --version


# Tipo GPU
!nvidia-smi


# Verifique se a GPU está disponível
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Dispositivo disponível:", device)


# Versão keras tensorflow
import keras
import tensorflow as tf

print("Versão tensorflow:", tf.__version__)
print("Versão keras:", keras.__version__)


# Base dados
df = pd.read_csv('Tweets.csv')
df


# Visualizando 5 primeiros dados
df.head()


# Visualizando 5 últimos dados
df.tail()


# Visualziando linhas e colunas
df.shape


# Info dados
df.info()


# Tipo de dados
df.dtypes





# Converter todos os valores da coluna "clean_text" para strings
df['text'] = df['text'].astype(str)


# Converter todos os valores da coluna "clean_text" para strings
df['selected_text'] = df['selected_text'].astype(str)


# Converter todos os valores da coluna "clean_text" para strings
df['sentiment'] = df['sentiment'].astype(str)


# Excluir coluna
df = df.drop(['textID'], axis=1)
df.head()


# Função para pré-processamento do texto
def preprocess_text(text):
    if isinstance(text, str):

        # Verificar se o texto é uma string
        # Converter texto para minúsculas
        text = text.lower()

        # Remover emojis
        emoji_pattern = re.compile("["
                                   u"\U0001F600-\U0001F64F"  # emoticons
                                   u"\U0001F300-\U0001F5FF"  # símbolos & pictogramas
                                   u"\U0001F680-\U0001F6FF"  # transporte & símbolos mapas
                                   u"\U0001F1E0-\U0001F1FF"  # bandeiras (iOS)
                                   u"\U00002500-\U00002BEF"  # caracteres chineses/japoneses coreanos unificados
                                   u"\U00002702-\U000027B0"
                                   u"\U00002702-\U000027B0"
                                   u"\U000024C2-\U0001F251"
                                   u"\U0001f926-\U0001f937"
                                   u"\U00010000-\U0010ffff"
                                   u"\u2640-\u2642"
                                   u"\u2600-\u2B55"
                                   u"\u200d"
                                   u"\u23cf"
                                   u"\u23e9"
                                   u"\u231a"
                                   u"\ufe0f"  # dingbats
                                   u"\u3030"
                                   "]+", flags=re.UNICODE)
        text = emoji_pattern.sub(r'', text)

        # Remover caracteres especiais
        text = re.sub(r'[^\w\s]', '', text)

        # Tokenizar o texto em palavras
        tokens = word_tokenize(text)

        # Remover stopwords
        stop_words = set(stopwords.words('english')) # Defina o idioma apropriado
        tokens = [word for word in tokens if word not in stop_words]

        # Lematização (redução das palavras às suas formas base)
        lemmatizer = WordNetLemmatizer() # Utilize o lematizador apropriado para o idioma
        tokens = [lemmatizer.lemmatize(word) for word in tokens]

        # Juntar as palavras de volta em texto
        preprocessed_text = ' '.join(tokens)
    else:
        preprocessed_text = np.nan  # Caso não seja uma string, mantenha o valor como NaN

    return preprocessed_text

# Aplicar a função de pré-processamento à coluna "clean_text"
df['clean_text_2'] = df['text'].apply(preprocess_text)


# Remover linhas com valores nulos na coluna 'clean_text_2'
df = df.dropna(subset=['clean_text_2'])


# Converter todos os valores da coluna "clean_text" para strings
df['clean_text_2'] = df['clean_text_2'].astype(str)


# Concatenar todos os textos em uma única string
all_text = ' '.join(df['clean_text_2'])


# Exibir o DataFrame resultante
df.head()





from sklearn.cluster import KMeans
from transformers import GemmaModel, GemmaTokenizer
from sklearn.feature_extraction.text import TfidfVectorizer


# Verifique se o GPU está disponível
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)


# login huggingface
!huggingface-cli login


# Base dados
df


# Importando biblioteca
from sklearn.preprocessing import LabelEncoder

# Codificação dos sentimentos em valores numéricos
encoder = LabelEncoder()
df['sentiment_encoded'] = encoder.fit_transform(df['sentiment'])

# Visualziando dataset
df


## Vetorizar os textos

# Importando bilioteca
from sklearn.feature_extraction.text import TfidfVectorizer

# Vetorizando
vetorizador = TfidfVectorizer()

# Treinamento
X = vetorizador.fit_transform(df.clean_text_2)

# Visualizando
vetorizador


import torch
from transformers import GemmaModel, GemmaConfig
from transformers import GemmaModel, GemmaTokenizer


# Carregar o modelo Gemma no dispositivo correto
model = GemmaModel.from_pretrained("google/gemma-2b-en").to(device)


# Tokenização
tokenizer = GemmaTokenizer.from_pretrained("google/gemma-2b")


# Supondo que 'dataset' é o seu conjunto de dados
texts = df['clean_text_2'].tolist()


# Tokenizar e codificar os textos
inputs = tokenizer(texts,
                   return_tensors="pt",
                   padding=True,
                   truncation=True,
                   max_length=512).to(device)


# Obter a representação vetorial dos textos
with torch.no_grad():
    outputs = model(**inputs)

# Use a última camada oculta como representação vetorial do texto
text_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()  # Move para CPU para clusterização


### Encontrar o valor cluster

# Importando biblioteca
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# Calcula a inércia para diferentes números de clusters
inertias = []
max_clusters = 10
for k in range(2, max_clusters + 1):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X)
    inertias.append(kmeans.inertia_)

# Encontra o ponto de cotovelo
diff = np.diff(inertias)
diff_r = diff[1:] / diff[:-1]

# Plotando o gráfico da inércia em função do número de clusters
plt.figure(figsize=(10, 5))
plt.plot(range(2, max_clusters + 1), inertias, marker='o')
plt.xlabel('Número de Clusters')
plt.ylabel('Inércia')
plt.title('Método do Cotovelo')
plt.xticks(np.arange(2, max_clusters + 1, step=1))
plt.grid(False)
plt.show()

# Encontrando o número ideal de clusters usando a regra do cotovelo
optimal_k = np.argmax(diff_r) + 2  # Adiciona 2 porque começamos em k=2
print("Número ideal de clusters (regra do cotovelo):", optimal_k)


## Modelo kmeans

# Clusterização com K-means usando o número ideal de clusters
kmeans = KMeans(n_clusters=optimal_k, random_state=42)

# Treinamento modelo
kmeans.fit(X)


















